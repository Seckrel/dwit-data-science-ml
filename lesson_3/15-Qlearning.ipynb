{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23fc395-f549-43ee-9c9b-688fb2347ceb",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3461085-490e-41f4-95ae-f5d4722567b3",
   "metadata": {},
   "source": [
    "Q-learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for an agent interacting with an environment. The agent learns a Q-value function that estimates the utility (or quality) of taking a specific action in a specific state to maximize long-term rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f36f3-576a-4b62-9a1d-ce93075de5c6",
   "metadata": {},
   "source": [
    "**Key Components of Q-Learning**\n",
    "- States (ùëÜ): The possible configurations or situations in which the agent can be.\n",
    "- Actions (A): The set of actions the agent can perform in each state.\n",
    "- Reward (R): The immediate feedback received after performing an action.\n",
    "- Q-value (Q(s,a)): A function that represents the expected cumulative reward of taking action a in state s and following the optimal policy afterward.\n",
    "\n",
    "---\n",
    "\n",
    "**The Q-Learning Algorithm**\n",
    "The Q-value is updated iteratively using the Bellman Equation:  \n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$  \n",
    "\n",
    "Where:  \n",
    "- Q(s,a): Current estimate of the Q-value for state s and action a.\n",
    "- Œ±: Learning rate (how much new information overrides old information).\n",
    "- R: Immediate reward received after taking action \n",
    "- Œ≥: Discount factor (how much future rewards are valued compared to immediate rewards).\n",
    "- $\\max_{a'} Q(s', a')$: Maximum Q-value for the next state \\(s'\\), over all possible actions \\(a'\\).\n",
    "- $\\gamma$: Discount factor, determining the importance of future rewards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e8468-750f-47a3-9296-29282571b492",
   "metadata": {},
   "source": [
    "simplified formula\n",
    "$$\n",
    "Q(s, a) = R(s, a) + \\gamma \\max_{a'} Q(s', a_{all}')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9a13d-7ace-4057-a80f-53236645b644",
   "metadata": {},
   "source": [
    "**Initialization:**\n",
    "1. Initialize\n",
    "    - Q(s,a) to arbitrary values (e.g., zero) for all state-action pairs.\n",
    "2. Action Selection:\n",
    "    - Use an exploration strategy (e.g.,œµ-greedy) to choose an action:\n",
    "        - With probability œµ, choose a random action (exploration).\n",
    "        - With probability 1‚àíœµ, choose the action with the highest Q-value (exploitation).\n",
    "3. Action Execution and Feedback:\n",
    "    - Perform the chosen action \n",
    "a, observe the new state s‚Ä≤and reward R.\n",
    "4. Q-value Update:\n",
    "    - Update Q(s,a) using the Bellman equation.\n",
    "5. Repeat:\n",
    "    - Continue until the agent learns an optimal policy or the process converges.\n",
    "\n",
    "---\n",
    "**Key Features of Q-Learning**\n",
    "- Model-Free: Q-learning does not require a model of the environment (e.g., transition probabilities).\n",
    "- Off-Policy: It learns the optimal policy independent of the agent's current behavior policy.\n",
    "- Convergence: Given enough exploration and a suitable learning rate, Q-learning converges to the optimal Q-values.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications of Q-Learning**\n",
    "- Robotics: Navigation and control.\n",
    "- Game playing: Agents learning to play board games or video games.\n",
    "- Operations research: Solving optimization problems.\n",
    "- Finance: Portfolio management and trading strategies.\n",
    "\n",
    "Q-learning serves as a foundation for more advanced reinforcement learning algorithms like Deep Q-Networks (DQN), where deep neural networks approximate the Q-value function for environments with large or continuous state spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d103be2-2808-4d92-9df5-f46ae8197cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203b3cf9-5bea-49b1-ab79-cb897d50f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple environment\n",
    "# States: 0 (start), 1, 2, 3 (goal)\n",
    "# Actions: 0 (left), 1 (right)\n",
    "rewards = [0, 0, 0, 1]  # Reward is 1 only at the goal state (3)\n",
    "num_states = len(rewards)\n",
    "num_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf796cd-41e5-47af-8d03-f535d6220e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table with zeros\n",
    "q_table = np.zeros((num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668648ac-d99a-4234-8d5c-81cf6dc0fa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be9607b-21aa-444b-b6ed-27efab465edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.2  # Exploration rate\n",
    "episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b110fd75-b64a-4827-9d72-0e0d7766177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state = 0  # Start state\n",
    "    while state != 3:  # Loop until reaching the goal\n",
    "        # Choose an action (epsilon-greedy)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice([0, 1])  # Explore: random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit: best action\n",
    "        \n",
    "        # Determine the next state\n",
    "        next_state = state + 1 if action == 1 else max(0, state - 1)\n",
    "        \n",
    "        # Update Q-value\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            rewards[next_state] + discount_factor * np.max(q_table[next_state]) - q_table[state, action]\n",
    "        )\n",
    "        \n",
    "        state = next_state  # Move to the next state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825e8928-df2f-4955-b42e-c1a4f5a4d901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-Table:\n",
      "[[0.72899977 0.81      ]\n",
      " [0.72899748 0.9       ]\n",
      " [0.8099861  1.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Display the learned Q-table\n",
    "print(\"Learned Q-Table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba66855-fc8e-4996-b74a-099e6b9c8f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the agent:\n",
      "Moved to state 1\n",
      "Moved to state 2\n",
      "Moved to state 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the trained agent\n",
    "state = 0\n",
    "print(\"\\nTesting the agent:\")\n",
    "while state != 3:\n",
    "    action = np.argmax(q_table[state])  # Pick the best action\n",
    "    state = state + 1 if action == 1 else max(0, state - 1)\n",
    "    print(f\"Moved to state {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9fea7-839d-452d-abce-9509486eddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
